{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911c3ae1c5c87a5b",
   "metadata": {},
   "source": [
    "# Exercise 6: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64b714f9b2c00c",
   "metadata": {},
   "source": [
    "## Task 1: Implementation of Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twoface/Documents/Passau/dlnlp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from gensim.downloader import load as gensim_load\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bd249ac0f512cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = gensim_load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307022694dda76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb(n_samples=100):\n",
    "    dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "    # How many samples per class\n",
    "    n_per_class = n_samples // 2\n",
    "\n",
    "    # Filter each class\n",
    "    pos = dataset.filter(lambda x: x[\"label\"] == 1).shuffle(seed=42).select(range(n_per_class))\n",
    "    neg = dataset.filter(lambda x: x[\"label\"] == 0).shuffle(seed=42).select(range(n_per_class))\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced = concatenate_datasets([pos, neg]).shuffle(seed=42)\n",
    "\n",
    "    texts = balanced[\"text\"]\n",
    "    labels = balanced[\"label\"]\n",
    "    return texts, labels\n",
    "\n",
    "texts, labels = load_imdb(n_samples=1000) # adjust n_samples based on your computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae4c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a HuggingFace tokenizer (e.g., bert-base-uncased)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize input text using HuggingFace tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input sentence.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of tokens.\n",
    "    \"\"\"\n",
    "    return hf_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8161194cf4545ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens, max_len=100):\n",
    "    \"\"\"\n",
    "    Convert tokens to GloVe vectors with padding/truncation\n",
    "    \n",
    "    Args:\n",
    "        tokens: list of string tokens\n",
    "        max_len: maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (max_len, embedding_dim)\n",
    "    \"\"\"\n",
    "    # Get embedding dimension from GloVe (should be 100 for glove-wiki-gigaword-100)\n",
    "    embedding_dim = glove.vector_size\n",
    "    \n",
    "    # Initialize result matrix with zeros (padding)\n",
    "    result = np.zeros((max_len, embedding_dim))\n",
    "    \n",
    "    # Process tokens up to max_len\n",
    "    for i, token in enumerate(tokens[:max_len]):\n",
    "        try:\n",
    "            # Get GloVe vector for the token\n",
    "            vector = glove[token.lower()]  # Convert to lowercase for better matching\n",
    "            result[i] = vector\n",
    "        except KeyError:\n",
    "            # Token not in GloVe vocabulary - leave as zero vector\n",
    "            # Alternatively, you could use a random vector or UNK token\n",
    "            pass\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b19bc049f502c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: The fine cast cannot uplift this routine tale of a secretary murdered by her married paramour. In fact there are more questions than answers in this one-sided tale of romance and murder; and since we are only provided with the prosecution's side, none of these questions will be answered. This is the type of fare that appeals to the \"He Woman, Man Hater\" clubs of America. As presented, it is the tale of an innocent woman who just happens to be \"caught up\" in a romance with a married, high-profile attorney. Is it possible that IF, she had not been two timing her boy friend and having an affair with a married man, the whole nasty murderous, sordid incident could have been avoided? When you watch this, don't worry about going to the 'fridge, you won't miss anything.\n",
      "Sample text length: 138\n",
      "(100, 100)\n",
      "[[-0.038194   -0.24487001  0.72812003 ... -0.1459      0.82779998\n",
      "   0.27061999]\n",
      " [ 0.50524002  0.49487001 -0.57381999 ... -0.41922     0.66611999\n",
      "  -0.1165    ]\n",
      " [-0.40184     0.64433002  0.15978    ...  0.054131    0.20832001\n",
      "   0.060862  ]\n",
      " ...\n",
      " [ 0.63256001 -0.12718    -0.084182   ... -0.30965999  0.22378001\n",
      "   0.038183  ]\n",
      " [-0.19103999  0.17601     0.36919999 ... -0.59680003  0.080843\n",
      "   0.27866   ]\n",
      " [-0.12135     0.15341    -0.014315   ... -0.50936002  0.5068\n",
      "   0.080349  ]]\n"
     ]
    }
   ],
   "source": [
    "# Test the vectorize function on a sample sentence\n",
    "sample_text = texts[0]\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"Sample text length:\", len(sample_text.split()))\n",
    "sample_tokens = sample_text.split()  # simple tokenization\n",
    "vectorized = vectorize(sample_tokens, max_len=100)\n",
    "print(vectorized.shape)\n",
    "print(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affacb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    \"text\": texts,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "train_df[\"tokens\"] = train_df[\"text\"].apply(tokenize)\n",
    "train_df[\"vectors\"] = train_df[\"tokens\"].apply(lambda x: vectorize(x, max_len=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef678097e15837ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.WQ = nn.Linear(d_model, d_k)\n",
    "        self.WK = nn.Linear(d_model, d_k)\n",
    "        self.WV = nn.Linear(d_model, d_k)\n",
    "\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.WQ(x)  # (batch_size, seq_len, d_k)\n",
    "        K = self.WK(x)  # (batch_size, seq_len, d_k)\n",
    "        V = self.WV(x)  # (batch_size, seq_len, d_k)\n",
    "        \n",
    "        K_transpose = K.transpose(-2, -1)  # (batch_size, d_k, seq_len)\n",
    "        attention_scores = torch.matmul(Q, K_transpose)  # (batch_size, seq_len, seq_len)\n",
    "        scaled_attention_scores = attention_scores / np.sqrt(self.d_k)  # Scale by sqrt(d_k)\n",
    "        attention_weights = F.softmax(scaled_attention_scores, dim=-1)  # (batch_size, seq_len, seq_len)        \n",
    "        context_vector = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c53e5b0a61c1e",
   "metadata": {},
   "source": [
    "## Task 2: Adding a Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59b5594f0e9dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationModel(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(d_model, d_k)\n",
    "        self.classifier = nn.Linear(d_k, 1)  # Final layer for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vector = self.attention(x) # shape : (batch_size, seq_len, d_k)\n",
    "        context_vector = context_vector.mean(dim=1)  # Aggregate over sequence length\n",
    "        logits = self.classifier(context_vector)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b59c6e05c7cb0a",
   "metadata": {},
   "source": [
    "## Task 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48a0bc8fb29053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_model(model, X, y, epochs=10, lr=1e-3, batch_size=8):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Outer progress bar for epochs\n",
    "    epoch_pbar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Inner progress bar for batches\n",
    "        batch_pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", \n",
    "                         leave=False, unit=\"batch\")\n",
    "        \n",
    "        for batch in batch_pbar:\n",
    "            inputs, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted_probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (predicted_probabilities > 0.5).float()\n",
    "            correct_predictions += (predictions.squeeze() == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "            \n",
    "            # Update batch progress bar with current loss\n",
    "            batch_pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        \n",
    "        # Update epoch progress bar with metrics\n",
    "        epoch_pbar.set_postfix({\n",
    "            \"Avg Loss\": f\"{avg_loss:.4f}\", \n",
    "            \"Accuracy\": f\"{accuracy:.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceebf114dda52d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1188/3391687891.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  X = torch.tensor(train_df[\"vectors\"].tolist(), dtype=torch.float32)\n",
      "Training:  10%|█         | 1/10 [00:01<00:12,  1.39s/epoch, Avg Loss=0.6876, Accuracy=4.0160]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 - Avg Loss: 0.6876, Accuracy: 4.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 2/10 [00:02<00:09,  1.23s/epoch, Avg Loss=0.6121, Accuracy=4.1120]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10 - Avg Loss: 0.6121, Accuracy: 4.1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 3/10 [00:03<00:09,  1.29s/epoch, Avg Loss=0.5493, Accuracy=4.1400]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10 - Avg Loss: 0.5493, Accuracy: 4.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 4/10 [00:05<00:07,  1.25s/epoch, Avg Loss=0.5223, Accuracy=4.2440]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10 - Avg Loss: 0.5223, Accuracy: 4.2440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 5/10 [00:06<00:05,  1.19s/epoch, Avg Loss=0.5093, Accuracy=4.2480]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10 - Avg Loss: 0.5093, Accuracy: 4.2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 6/10 [00:07<00:04,  1.13s/epoch, Avg Loss=0.4904, Accuracy=4.3520]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10 - Avg Loss: 0.4904, Accuracy: 4.3520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 7/10 [00:08<00:03,  1.13s/epoch, Avg Loss=0.4715, Accuracy=4.3180]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10 - Avg Loss: 0.4715, Accuracy: 4.3180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 8/10 [00:09<00:02,  1.06s/epoch, Avg Loss=0.4593, Accuracy=4.2120]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10 - Avg Loss: 0.4593, Accuracy: 4.2120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 9/10 [00:10<00:01,  1.01s/epoch, Avg Loss=0.4544, Accuracy=4.2180]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10 - Avg Loss: 0.4544, Accuracy: 4.2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:11<00:00,  1.11s/epoch, Avg Loss=0.4420, Accuracy=4.2680]\n",
      "Training: 100%|██████████| 10/10 [00:11<00:00,  1.11s/epoch, Avg Loss=0.4420, Accuracy=4.2680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10 - Avg Loss: 0.4420, Accuracy: 4.2680\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100  # GloVe embedding dimension\n",
    "d_model = EMBEDDING_DIM\n",
    "d_k = 64 # try different values for d_k\n",
    "model = BinaryClassificationModel(d_model=d_model, d_k=d_k)\n",
    "\n",
    "X = torch.tensor(train_df[\"vectors\"].tolist(), dtype=torch.float32)\n",
    "y = torch.tensor(train_df[\"label\"].tolist(), dtype=torch.float32).unsqueeze(1)  # Ensure y is of shape (batch_size, 1)\n",
    "train_model(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b553763845ec",
   "metadata": {},
   "source": [
    "## Task 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed06b756f3828381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    tokens = tokenize(text)\n",
    "    vectorized = vectorize(tokens, max_len=100)\n",
    "    vectorized_tensor = torch.tensor(vectorized, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(vectorized_tensor)\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        prediction = (probabilities > 0.5).float()\n",
    "    return prediction.item(), probabilities.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10aff316ffbf98cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5533190369606018)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This movie was fantastic and full of suspense!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb0346227e20cadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.4657359719276428)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This movie was bad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
