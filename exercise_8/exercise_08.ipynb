{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uOYcTVi6vsLJ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from random import randrange, shuffle\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jxU5OQIhY_0W"
   },
   "source": [
    "# Exercise 8: Encoder Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nr7ZIX0JZGLE"
   },
   "source": [
    "### Toy example based\n",
    "This code is based on code developed by Dong-Hyun Lee.\n",
    "\n",
    "\n",
    "Please, read carefully the code, since it could help you in the subsequent programming tasks. While you read the text, try to answer each of the questions that are presented in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8iN-070y7KU3"
   },
   "outputs": [],
   "source": [
    "# Example of a single review. This review is a modified version of the first positive review of the dataset that we are using\n",
    "# Note that is a single review\n",
    "review = (\"Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \\\"Teachers\\\".\"\n",
    "        \"My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \\\"Teachers\\\".\"\n",
    "         \"The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students.\"\n",
    "         \"When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled  at  High.\"\n",
    "         \"A classic line: INSPECTOR: I'm here to sack one of your teachers, STUDENT: Welcome to Bromwell High.\"\n",
    "         \"I expect that many adults of my age think that Bromwell High is far fetched.\"\n",
    "         \"What a pity that it isn't!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hT4GlPYU0kgf"
   },
   "outputs": [],
   "source": [
    "# Basic tokenizer - you should use your tokenizer from previous exercises or an improved version of the following code, but you can use the following code as a starting point\n",
    "sentences = re.sub(\"[,!?\\\\-]\", '', review.lower()).split('.')  #  How are we splitting differentiating sentence?\n",
    "vocab = list(set(\" \".join(sentences).split()))\n",
    "\n",
    "# Importantly, we are adding some special tokens to the vocabulary.\n",
    "# From now, start thinking what is tok1, tok2, tok3, and tok4 (based on there usage in create_B)\n",
    "TOK_1 = '[tok1]'\n",
    "TOK_2 = '[tok2]'\n",
    "TOK_3 = '[tok3]'\n",
    "TOK_4 = '[tok4]'\n",
    "tokens_2_index_dict = {TOK_1: 0, TOK_2: 1, TOK_3: 2, TOK_4: 3}\n",
    "\n",
    "# The following loops should be familiar to you, since we have been doing this from exercise #2\n",
    "init_index = len(tokens_2_index_dict)\n",
    "\n",
    "# Create two dictionaries one for mapping index (ids) to tokens and one for tokens to index\n",
    "for i, token in enumerate(vocab):\n",
    "    tokens_2_index_dict[token] = i + init_index\n",
    "\n",
    "index_2_token = {}\n",
    "for i, token in enumerate(tokens_2_index_dict):\n",
    "    index_2_token[i] = token\n",
    "\n",
    "vocab_size = len(tokens_2_index_dict)\n",
    "\n",
    "sentences_2_tokens_lst = []\n",
    "for sentence in sentences:\n",
    "\n",
    "    lst_temporal = []\n",
    "    for word in sentence.split():\n",
    "        lst_temporal.append(tokens_2_index_dict[word])\n",
    "\n",
    "    sentences_2_tokens_lst.append(lst_temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell high is a cartoon comedy',\n",
       " ' it ran at the same time as some other programs about school life such as \"teachers\"',\n",
       " 'my 35 years in the teaching profession lead me to believe that bromwell high\\'s satire is much closer to reality than is \"teachers\"',\n",
       " \"the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students\",\n",
       " 'when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high',\n",
       " \"a classic line: inspector: i'm here to sack one of your teachers student: welcome to bromwell high\",\n",
       " 'i expect that many adults of my age think that bromwell high is far fetched',\n",
       " \"what a pity that it isn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RjYB8A2-sEDA"
   },
   "source": [
    "## Theoretical part 1 - code related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_X = 100 # what is X? Max length of the sequence\n",
    "size_B = 6 # What is B? Size of the batch to generate\n",
    "max_pred_M = 10  # what is M? Maximum number of tokens to mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XWlnMGJc0rRa"
   },
   "outputs": [],
   "source": [
    "def create_B(maxlen_X=maxlen_X, size_B=size_B, max_pred_M=max_pred_M):\n",
    "    assert size_B % 2 == 0, \"size_B should be even\"\n",
    "    \n",
    "    lst_B = []\n",
    "    positive_pair_X = 0\n",
    "    negative_pair_X = 0\n",
    "    number_sentences = len(sentences)\n",
    "\n",
    "    while positive_pair_X != size_B/2 or negative_pair_X != size_B/2: # we want 50% of something positive and 50% of something negative\n",
    "        \n",
    "        # Start by taking two random sentences from the list of sentences index\n",
    "        tokens_X1_index = randrange(number_sentences)\n",
    "        tokens_X2_index = randrange(number_sentences)\n",
    "\n",
    "        # Get the relevant sentences (as token list)\n",
    "        tokens_X1 = sentences_2_tokens_lst[tokens_X1_index]\n",
    "        tokens_X2 = sentences_2_tokens_lst[tokens_X2_index]\n",
    "\n",
    "        #What are we doing here? WHy do we need to do this?\n",
    "        # Build a sequence like this : TOK2 + [sentence1] + TOK3 + [sentence2] + TOK3 (Tok2 is likely [START] or [CLS] and tok3 is likely [SEP])\n",
    "        # Rectification : TOK1 is [PAD]\n",
    "        input_ids_X = [tokens_2_index_dict[TOK_2]] + tokens_X1 + [tokens_2_index_dict[TOK_3]] + tokens_X2 + [tokens_2_index_dict[TOK_3]]\n",
    "\n",
    "        # Get the maximum number of tokens to mask (we will pick 15% of the tokens of the sequence, with a minimum of 1)\n",
    "        int_max_number_pred = max(1,int(round(len(input_ids_X) * 0.15)))\n",
    "        \n",
    "        # On cap a max_pred_M, au cas ou le nombre de tokens a masquer est trop grand\n",
    "        n_pred_M =  min(max_pred_M, int_max_number_pred) # max - 15% of tokens in one STU\n",
    "        \n",
    "        # every token can be M?\n",
    "        # We add a candidate list of positions for Masking tokens\n",
    "        cand_M_pos = []\n",
    "        # loop through the input_ids_X and find the positions of tokens that are not TOK_2 or TOK_3\n",
    "        for i, token in enumerate(input_ids_X):\n",
    "            if token != tokens_2_index_dict[TOK_2] and token != tokens_2_index_dict[TOK_3]:\n",
    "                cand_M_pos.append(i)\n",
    "\n",
    "        # Shuffle the candidate positions to randomly select which tokens to mask\n",
    "        shuffle(cand_M_pos) # Why do we need to shuffle this?\n",
    "        M_tokens = []\n",
    "        M_pos = []\n",
    "\n",
    "        #What is this loop iterating?\n",
    "        # We select the first n_pred_M positions from the shuffled candidate positions\n",
    "        # and replace the corresponding tokens in input_ids_X with TOK_4 (which is likely [MASK])\n",
    "        # We also store the original tokens and their positions in M_tokens and M_pos\n",
    "        for pos in cand_M_pos[:n_pred_M]:\n",
    "            M_pos.append(pos)\n",
    "            M_tokens.append(input_ids_X[pos])\n",
    "            # TOK4 = [MASK]\n",
    "            input_ids_X[pos] = tokens_2_index_dict[TOK_4]\n",
    "\n",
    "\n",
    "        # Why do we need to pad input_ids_X?\n",
    "        # We add padding just in case the input_ids_X is shorter than maxlen_X\n",
    "        n_pad = maxlen_X - len(input_ids_X)\n",
    "        input_ids_X.extend([tokens_2_index_dict[TOK_1]] * n_pad)\n",
    "\n",
    "\n",
    "        # What would happen if we do not have the conditional?\n",
    "        # If the number of tokens in M_tokens is less than max_pred_M, we pad it with TOK_1\n",
    "        if max_pred_M > n_pred_M:\n",
    "            n_pad = max_pred_M - n_pred_M\n",
    "            M_tokens.extend([tokens_2_index_dict[TOK_1]] * n_pad)\n",
    "            \n",
    "            # BUG HERE! We should pad M_pos with -1 or 0, not TOK_1\n",
    "            # M_pos.extend([tokens_2_index_dict[TOK_1]] * n_pad\n",
    "            M_pos.extend([-1] * n_pad)\n",
    "\n",
    "        # What are we verifying with this conditional?\n",
    "        if tokens_X1_index + 1 == tokens_X2_index and positive_pair_X < size_B/2:\n",
    "            lst_B.append([input_ids_X, M_tokens, M_pos, True])\n",
    "            positive_pair_X += 1\n",
    "\n",
    "        elif tokens_X1_index + 1 != tokens_X2_index and negative_pair_X < size_B/2:\n",
    "            lst_B.append([input_ids_X, M_tokens, M_pos, False])\n",
    "            negative_pair_X += 1\n",
    "\n",
    "    return lst_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BAUcEjBD01zF"
   },
   "outputs": [],
   "source": [
    "B = create_B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_Xs, M_tokens, M_pos, isNext = map(torch.LongTensor, zip(*B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Tasks: Implementing an Encoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, nhead=4, num_layers=8, dim_feedforward=128, max_len=100):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=dim_feedforward) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.nsp_head = nn.Linear(embed_dim, 2)  # For next sentence prediction\n",
    "        self.mlm_head = nn.Linear(embed_dim, vocab_size)  # For masked language modeling\n",
    "        \n",
    "    def forward(self, input_ids, M_pos):\n",
    "        # input_ids: shape (batch_size, seq_len)\n",
    "        batch_size = input_ids.size(0)\n",
    "        embeddings = self.word_embedding(input_ids)\n",
    "        positions = torch.arange(0, input_ids.size(1), device=input_ids.device).unsqueeze(0)\n",
    "        positional_embeddings = self.positional_embedding(positions)\n",
    "        x = embeddings + positional_embeddings\n",
    "        \n",
    "        for encoder in self.encoder_layers:\n",
    "            x = encoder(x)\n",
    "        \n",
    "        cls_token = x[:, 0, :]  # Assuming the first token is the CLS token\n",
    "        nsp_output = self.nsp_head(cls_token)\n",
    "        \n",
    "        mlm_output_list = []\n",
    "        for i in range(batch_size):\n",
    "            sentence = x[i]\n",
    "            mlm_output_list.append(sentence[M_pos[i]])\n",
    "            \n",
    "        mlm_output_tensor = torch.cat(mlm_output_list, dim=0)\n",
    "        \n",
    "        mlm_output = self.mlm_head(mlm_output_tensor)\n",
    "        return nsp_output, mlm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
