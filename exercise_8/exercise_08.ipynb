{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "uOYcTVi6vsLJ"
   },
   "source": [
    "import re\n",
    "from random import randrange, shuffle\n",
    "import torch\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jxU5OQIhY_0W"
   },
   "source": "# Exercise 8: Encoder Model"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nr7ZIX0JZGLE"
   },
   "source": [
    "### Toy example based\n",
    "This code is based on code developed by Dong-Hyun Lee.\n",
    "\n",
    "\n",
    "Please, read carefully the code, since it could help you in the subsequent programming tasks. While you read the text, try to answer each of the questions that are presented in the comments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8iN-070y7KU3"
   },
   "source": [
    "# Example of a single review. This review is a modified version of the first positive review of the dataset that we are using\n",
    "# Note that is a single review\n",
    "review = (\"Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \\\"Teachers\\\".\"\n",
    "        \"My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \\\"Teachers\\\".\"\n",
    "         \"The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students.\"\n",
    "         \"When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled  at  High.\"\n",
    "         \"A classic line: INSPECTOR: I'm here to sack one of your teachers, STUDENT: Welcome to Bromwell High.\"\n",
    "         \"I expect that many adults of my age think that Bromwell High is far fetched.\"\n",
    "         \"What a pity that it isn't!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hT4GlPYU0kgf"
   },
   "source": [
    "# Basic tokenizer - you should use your tokenizer from previous exercises or an improved version of the following code, but you can use the following code as a starting point\n",
    "sentences = re.sub(\"[,!?\\\\-]\", '', review.lower()).split('.')  #  How are we splitting differentiating sentence?\n",
    "vocab = list(set(\" \".join(sentences).split()))\n",
    "\n",
    "# Importantly, we are adding some special tokens to the vocabulary.\n",
    "# From now, start thinking what is tok1, tok2, tok3, and tok4 (based on there usage in create_B)\n",
    "TOK_1 = '[tok1]'\n",
    "TOK_2 = '[tok2]'\n",
    "TOK_3 = '[tok3]'\n",
    "TOK_4 = '[tok4]'\n",
    "tokens_2_index_dict = {TOK_1: 0, TOK_2: 1, TOK_3: 2, TOK_4: 3}\n",
    "\n",
    "# The following loops should be familiar to you, since we have been doing this from exercise #2\n",
    "init_index = len(tokens_2_index_dict)\n",
    "\n",
    "# Create two dictionaries one for mapping index (ids) to tokens and one for tokens to index\n",
    "for i, token in enumerate(vocab):\n",
    "    tokens_2_index_dict[token] = i + init_index\n",
    "\n",
    "index_2_token = {}\n",
    "for i, token in enumerate(tokens_2_index_dict):\n",
    "    index_2_token[i] = token\n",
    "\n",
    "vocab_size = len(tokens_2_index_dict)\n",
    "\n",
    "sentences_2_tokens_lst = []\n",
    "for sentence in sentences:\n",
    "\n",
    "    lst_temporal = []\n",
    "    for word in sentence.split():\n",
    "        lst_temporal.append(tokens_2_index_dict[word])\n",
    "\n",
    "    sentences_2_tokens_lst.append(lst_temporal)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sentences",
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RjYB8A2-sEDA"
   },
   "source": [
    "## Theoretical part 1 - code related"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "maxlen_X = 100 # what is X?\n",
    "size_B = 6 # What is B?\n",
    "max_pred_M = 10  # what is M?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XWlnMGJc0rRa"
   },
   "source": [
    "def create_B(maxlen_X=maxlen_X, size_B=size_B, max_pred_M=max_pred_M):\n",
    "    assert size_B % 2 == 0, \"size_B should be even\"\n",
    "    \n",
    "    lst_B = []\n",
    "    positive_pair_X = 0\n",
    "    negative_pair_X = 0\n",
    "    number_sentences = len(sentences)\n",
    "\n",
    "    while positive_pair_X != size_B/2 or negative_pair_X != size_B/2: # we want 50% of something positive and 50% of something negative\n",
    "        tokens_X1_index = randrange(number_sentences)\n",
    "        tokens_X2_index = randrange(number_sentences)\n",
    "\n",
    "        tokens_X1 = sentences_2_tokens_lst[tokens_X1_index]\n",
    "        tokens_X2 = sentences_2_tokens_lst[tokens_X2_index]\n",
    "\n",
    "        #What are we doing here? WHy do we need to do this?\n",
    "        input_ids_X = [tokens_2_index_dict[TOK_2]] + tokens_X1 + [tokens_2_index_dict[TOK_3]] + tokens_X2 + [tokens_2_index_dict[TOK_3]]\n",
    "\n",
    "        int_max_number_pred = max(1,int(round(len(input_ids_X) * 0.15)))\n",
    "        n_pred_M =  min(max_pred_M, int_max_number_pred) # max - 15% of tokens in one STU\n",
    "        \n",
    "        # every token can be M?\n",
    "        cand_M_pos = []\n",
    "        for i, token in enumerate(input_ids_X):\n",
    "            if token != tokens_2_index_dict[TOK_2] and token != tokens_2_index_dict[TOK_3]:\n",
    "                cand_M_pos.append(i)\n",
    "\n",
    "        shuffle(cand_M_pos) # Why do we need to shuffle this?\n",
    "        M_tokens = []\n",
    "        M_pos = []\n",
    "\n",
    "        #What is this loop iterating?\n",
    "        for pos in cand_M_pos[:n_pred_M]:\n",
    "            M_pos.append(pos)\n",
    "            M_tokens.append(input_ids_X[pos])\n",
    "            input_ids_X[pos] = tokens_2_index_dict[TOK_4]\n",
    "\n",
    "\n",
    "        # Why do we need to pad input_ids_X?\n",
    "        n_pad = maxlen_X - len(input_ids_X)\n",
    "        input_ids_X.extend([tokens_2_index_dict[TOK_1]] * n_pad)\n",
    "\n",
    "\n",
    "        # What would happen if we do not have the conditional?\n",
    "        if max_pred_M > n_pred_M:\n",
    "            n_pad = max_pred_M - n_pred_M\n",
    "            M_tokens.extend([tokens_2_index_dict[TOK_1]] * n_pad)\n",
    "            M_pos.extend([tokens_2_index_dict[TOK_1]] * n_pad)\n",
    "\n",
    "        # What are we verifying with this conditional?\n",
    "        if tokens_X1_index + 1 == tokens_X2_index and positive_pair_X < size_B/2:\n",
    "            lst_B.append([input_ids_X, M_tokens, M_pos, True])\n",
    "            positive_pair_X += 1\n",
    "\n",
    "        elif tokens_X1_index + 1 != tokens_X2_index and negative_pair_X < size_B/2:\n",
    "            lst_B.append([input_ids_X, M_tokens, M_pos, False])\n",
    "            negative_pair_X += 1\n",
    "\n",
    "    return lst_B"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BAUcEjBD01zF"
   },
   "source": [
    "B = create_B()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "input_ids_Xs, M_tokens, M_pos, isNext = map(torch.LongTensor, zip(*B))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Programming Tasks: Implementing an Encoder Model"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, nhead=4, num_layers=8, dim_feedforward=128, max_len=100):\n",
    "        super().__init__()\n",
    "        # todo\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # todo"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Training",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
